# ============================================================================
# VALIDATED PATTERNS DATABASE
# ============================================================================
# Learning database that grows as Foundation Agent validates real systems
# Stores proven patterns, measurement data, and accumulated wisdom
# Last Updated: 2025-10-01
# Purpose: Cross-session learning and pattern recognition
# ============================================================================

version: "1.0"
description: "Accumulated validated knowledge from real-world system analysis"

# ============================================================================
# SCHEMA DEFINITION
# ============================================================================
# Each validated pattern should follow this structure:

pattern_schema:
  pattern_id: "unique-identifier"
  constraint: "The specific constraint analyzed"
  classification: "PHYSICS | CONSTRUCT"
  
  # Core pattern information
  system_type: "web-app | mobile | distributed | embedded | etc."
  domain: "e-commerce | fintech | social | saas | etc."
  
  # Validation evidence
  validation_method: "How this was validated (profiling, measurement, etc.)"
  measurement_data: "Actual numbers/metrics"
  confidence: 0.0-1.0  # How certain are we?
  
  # Physics grounding
  physics_basis: "Which physical law/constraint applies"
  physics_value: "Expected value based on physics"
  measured_value: "Actual measured value"
  deviation: "How close to physics prediction"
  
  # Impact metrics
  f_score_before: "Foundation score before analysis"
  f_score_after: "Foundation score after analysis"
  f_score_improvement: "Delta improvement"
  delusion_coefficient_before: "Initial delusion"
  delusion_coefficient_after: "Reduced delusion"
  
  # Actionable insights
  architectural_recommendation: "What pattern to use"
  common_misconception: "What people often get wrong"
  optimization_applied: "What actually fixed it"
  
  # Metadata
  validated_by: "perception | architecture | execution | adaptation"
  timestamp: "ISO 8601 timestamp"
  system_context: "Brief description of analyzed system"
  
  # Learning
  related_patterns: ["List of related pattern_ids"]
  lessons_learned: "Key takeaways for future analysis"

# ============================================================================
# SEED PATTERNS (High-Confidence Established Knowledge)
# ============================================================================
# These patterns are proven across many systems
# Start with these, add more through validation

seed_patterns:
  
  # =========================================
  # NETWORK LATENCY PATTERNS
  # =========================================
  
  - pattern_id: "network-latency-001"
    constraint: "API response time >100ms"
    classification: "PHYSICS"
    
    system_type: "web-app"
    domain: "general"
    
    validation_method: "Network profiling across 100+ applications"
    measurement_data: "Speed of light + TCP handshake + processing"
    confidence: 1.0
    
    physics_basis: "Speed of light (1ms per 300km) + TCP overhead (1.5 RTT)"
    physics_value: "Varies by distance: local=1ms, cross-country=50ms, cross-ocean=150ms"
    
    architectural_recommendation: "Data locality, caching, CDN, edge computing"
    common_misconception: "Can optimize network latency to near-zero"
    
    lessons_learned: "Network calls dominate response time in distributed systems. Always measure where time is spent before optimizing 'business logic'."
    related_patterns: ["network-latency-002", "cache-effectiveness-001"]
  
  - pattern_id: "network-latency-002"
    constraint: "Microservices inter-service latency"
    classification: "PHYSICS"
    
    system_type: "distributed"
    domain: "microservices"
    
    validation_method: "Service mesh tracing (Istio, Linkerd)"
    measurement_data: "Typical: 5-20ms per hop, 10+ hops = 50-200ms total"
    confidence: 0.95
    
    physics_basis: "Network latency + serialization + deserialization"
    physics_value: "Minimum 1-5ms per service call within same datacenter"
    
    architectural_recommendation: "Minimize service hops, batch requests, async where possible"
    common_misconception: "Microservices add 'negligible' latency"
    optimization_applied: "Reduced 15 service hops to 3, latency 200ms → 40ms"
    
    lessons_learned: "Each service boundary adds real latency. Microservices trade latency for other benefits. Measure before committing to architecture."
    related_patterns: ["network-latency-001", "microservices-overhead-001"]
  
  # =========================================
  # DATABASE QUERY PATTERNS
  # =========================================
  
  - pattern_id: "database-query-001"
    constraint: "Database query >100ms"
    classification: "PHYSICS"
    
    system_type: "web-app"
    domain: "general"
    
    validation_method: "Query profiling (EXPLAIN ANALYZE)"
    measurement_data: "HDD seeks: 10ms per seek, SSD: 100μs, Network: +RTT"
    confidence: 0.98
    
    physics_basis: "Disk I/O, memory access times, network latency"
    physics_value: "Indexed query on SSD: <10ms, HDD: 10-100ms, Full table scan: seconds"
    
    architectural_recommendation: "Proper indexing, query optimization, caching, connection pooling"
    common_misconception: "Database is slow because of 'complex queries'"
    optimization_applied: "Added index: 2000ms → 15ms"
    
    lessons_learned: "Most 'slow database' issues are missing indexes or N+1 queries, not database being inherently slow. Measure actual query time."
    related_patterns: ["database-n+1-001", "cache-effectiveness-001"]
  
  - pattern_id: "database-n+1-001"
    constraint: "ORM generating excessive queries"
    classification: "CONSTRUCT"
    
    system_type: "web-app"
    domain: "general"
    
    validation_method: "SQL query logging and counting"
    measurement_data: "1 query becomes 1+N queries (N=items in result)"
    confidence: 1.0
    
    physics_basis: "Each query has fixed network + I/O overhead"
    physics_value: "1 query at 10ms vs N queries at 10ms each"
    measured_value: "100 items: 1 query (10ms) vs 101 queries (1010ms)"
    
    architectural_recommendation: "Eager loading, query batching, denormalization"
    common_misconception: "ORM abstracts away performance concerns"
    optimization_applied: "Added eager loading: 101 queries → 2 queries, 1000ms → 20ms"
    
    lessons_learned: "ORMs hide database access patterns. Always profile query count, not just individual query speed."
    related_patterns: ["database-query-001", "orm-abstraction-cost-001"]
  
  # =========================================
  # MEMORY & CACHE PATTERNS
  # =========================================
  
  - pattern_id: "cache-effectiveness-001"
    constraint: "Repeated computation/data fetching"
    classification: "PHYSICS"
    
    system_type: "general"
    domain: "general"
    
    validation_method: "Cache hit rate monitoring"
    measurement_data: "Cache hit: <1ms, Cache miss + fetch: 50-1000ms"
    confidence: 0.99
    
    physics_basis: "Memory hierarchy: L1 cache (1ns) → RAM (100ns) → SSD (100μs) → Network (1-100ms)"
    physics_value: "Cache hit = memory speed, miss = source speed"
    
    architectural_recommendation: "Cache frequently accessed data, TTL based on change rate"
    common_misconception: "Caching is premature optimization"
    optimization_applied: "Added Redis cache: 95% hit rate, avg response 200ms → 15ms"
    
    lessons_learned: "Caching is not optimization, it's working WITH physics. Memory is 1000× faster than network."
    related_patterns: ["memory-hierarchy-001", "network-latency-001"]
  
  - pattern_id: "memory-hierarchy-001"
    constraint: "Random access patterns causing slowness"
    classification: "PHYSICS"
    
    system_type: "general"
    domain: "data-processing"
    
    validation_method: "CPU profiling, cache miss counters"
    measurement_data: "L1 hit: 1ns, L2: 10ns, RAM: 100ns, SSD: 100μs (6 orders of magnitude)"
    confidence: 1.0
    
    physics_basis: "Memory hierarchy access times"
    physics_value: "Sequential access: ~10GB/s, Random access: ~100MB/s (100× slower)"
    
    architectural_recommendation: "Sequential access patterns, batch processing, data locality"
    common_misconception: "RAM is 'fast enough', access patterns don't matter"
    optimization_applied: "Reordered data structures for sequential access: 10min → 30sec"
    
    lessons_learned: "Data structure layout affects performance as much as algorithm choice. Cache locality matters."
    related_patterns: ["cache-effectiveness-001"]
  
  # =========================================
  # HUMAN COGNITION PATTERNS
  # =========================================
  
  - pattern_id: "working-memory-001"
    constraint: "UI complexity causing user errors"
    classification: "PHYSICS"
    
    system_type: "web-app"
    domain: "user-interface"
    
    validation_method: "User testing, error rate tracking"
    measurement_data: "Error rate increases exponentially with elements >7"
    confidence: 0.95
    
    physics_basis: "Human working memory: 7±2 chunks (Miller's Law)"
    physics_value: "Users can handle ~5-9 items simultaneously"
    
    architectural_recommendation: "Progressive disclosure, chunking, categorization"
    common_misconception: "Users can handle complex UIs with training"
    optimization_applied: "Reduced form fields 25 → 8 grouped sections, completion rate 40% → 85%"
    
    lessons_learned: "Human cognition is physics. Can't train away working memory limits. Design must accommodate."
    related_patterns: ["cognitive-load-001"]
  
  # =========================================
  # ARCHITECTURAL PATTERNS
  # =========================================
  
  - pattern_id: "microservices-overhead-001"
    constraint: "Microservices architecture slower than monolith"
    classification: "CONSTRUCT"
    
    system_type: "distributed"
    domain: "architecture"
    
    validation_method: "Distributed tracing, latency profiling"
    measurement_data: "Monolith: 20ms avg, Microservices: 150ms avg (same business logic)"
    confidence: 0.90
    
    physics_basis: "Network calls have fixed overhead (serialization, network, deserialization)"
    physics_value: "Each service hop: 5-20ms minimum"
    measured_value: "8 service hops × 15ms = 120ms overhead"
    
    architectural_recommendation: "Use microservices for operational benefits, not performance"
    common_misconception: "Microservices improve performance"
    optimization_applied: "Consolidated 3 chatty services into 1: 150ms → 45ms"
    
    lessons_learned: "Microservices are organizational pattern, not performance optimization. They trade latency for other benefits (scaling, deployment)."
    related_patterns: ["network-latency-002", "monolith-performance-001"]
  
  - pattern_id: "premature-optimization-001"
    constraint: "Complex optimization with no measurement"
    classification: "CONSTRUCT"
    
    system_type: "general"
    domain: "development-practice"
    
    validation_method: "Before/after profiling"
    measurement_data: "Optimized function: 0.5ms → 0.3ms (0.2ms saved), Cost: 2 weeks dev time"
    confidence: 0.85
    
    physics_basis: "Optimization only matters for code in critical path"
    physics_value: "Optimize what takes >1% of total time"
    measured_value: "Optimized code executed 10 times/request, saved 2ms total from 200ms request"
    
    architectural_recommendation: "Profile first, optimize hotspots only"
    common_misconception: "Code optimization always improves system performance"
    optimization_applied: "Abandoned premature optimization, focused on network calls: 200ms → 50ms"
    
    lessons_learned: "Without measurement, optimization is guessing. 90% of time in 10% of code. Find the 10% first."
    related_patterns: ["profiling-first-001"]

# ============================================================================
# LEARNING HISTORY
# ============================================================================
# This section grows as Foundation Agent validates real systems
# Each validation adds an entry here

learning_history:
  # Format for each entry:
  # - validation_id: "unique-id"
  #   timestamp: "ISO 8601"
  #   system_analyzed: "Brief description"
  #   constraints_identified: 15
  #   constraints_validated: 12
  #   f_score_improvement: 0.31
  #   new_patterns_discovered: ["pattern-id-1", "pattern-id-2"]
  #   key_insights: "What was learned"

  # Example entry (this would be added by Adaptation Agent):
  # - validation_id: "validation-20251001-143022"
  #   timestamp: "2025-10-01T14:30:22Z"
  #   system_analyzed: "E-commerce checkout flow"
  #   constraints_identified: 12
  #   constraints_validated: 10
  #   f_score_before: 0.35
  #   f_score_after: 0.72
  #   f_score_improvement: 0.37
  #   delusion_coefficient_before: 0.65
  #   delusion_coefficient_after: 0.28
  #   new_patterns_discovered: []
  #   reinforced_patterns: ["network-latency-001", "cache-effectiveness-001"]
  #   key_insights: "Team assumed 'complex business logic' was bottleneck. Reality: 80% time in uncached database queries. Added caching, 300ms → 60ms."

# Initial state: empty (will grow through usage)
# Adaptation Agent appends to this list after each validation

# ============================================================================
# PATTERN CONFIDENCE TRACKING
# ============================================================================
# Track how often patterns are validated (increases confidence)

pattern_validation_counts:
  # This tracks how many times each pattern has been confirmed
  # Format: pattern_id: validation_count
  # Higher count = higher confidence
  
  # Seed patterns start with 0 (based on prior knowledge, not this system's validations)
  "network-latency-001": 0
  "network-latency-002": 0
  "database-query-001": 0
  "database-n+1-001": 0
  "cache-effectiveness-001": 0
  "memory-hierarchy-001": 0
  "working-memory-001": 0
  "microservices-overhead-001": 0
  "premature-optimization-001": 0
  
  # As system validates more, this grows:
  # "network-latency-001": 15  ← Validated 15 times
  # "custom-pattern-001": 3    ← New pattern, validated 3 times

# ============================================================================
# DOMAIN-SPECIFIC PATTERN CLUSTERS
# ============================================================================
# Patterns tend to cluster by domain
# This helps Perception Agent quickly find relevant patterns

pattern_clusters:
  web_applications:
    common_patterns: 
      - "network-latency-001"
      - "database-query-001"
      - "database-n+1-001"
      - "cache-effectiveness-001"
      - "working-memory-001"
    common_delusions:
      - "Complex business logic is slow (usually: database/network)"
      - "Need more CPU (usually: waiting on I/O)"
      - "Users want all features visible (working memory limits)"
  
  microservices:
    common_patterns:
      - "network-latency-002"
      - "microservices-overhead-001"
    common_delusions:
      - "Microservices improve performance (organizational pattern)"
      - "Service boundaries are free (network cost)"
  
  mobile_applications:
    common_patterns:
      - "working-memory-001"
      - "network-latency-001"
    common_delusions:
      - "Users will wait for slow loads (3 seconds = bounce)"
      - "Rich features on mobile (battery/bandwidth constraints)"
  
  data_processing:
    common_patterns:
      - "memory-hierarchy-001"
      - "cache-effectiveness-001"
    common_delusions:
      - "Algorithm complexity matters most (data locality often dominates)"
      - "Distributed = faster (coordination overhead)"

# ============================================================================
# ANTI-PATTERNS (COMMON MISTAKES)
# ============================================================================
# Track commonly seen mistakes for faster recognition

common_anti_patterns:
  - anti_pattern_id: "optimization-without-measurement"
    description: "Optimizing code without profiling first"
    frequency: "Very common"
    typical_cost: "Weeks of developer time on 1% impact code"
    solution: "Profile, find hotspots, optimize only what matters"
    related_patterns: ["premature-optimization-001"]
  
  - anti_pattern_id: "assumed-bottleneck"
    description: "Assuming bottleneck without measurement"
    frequency: "Extremely common (70-80% of cases)"
    typical_cost: "Months optimizing wrong thing"
    solution: "Measure where time actually spent (usually: I/O, network, cache misses)"
    related_patterns: ["profiling-first-001"]
  
  - anti_pattern_id: "cargo-cult-architecture"
    description: "Using pattern because others use it, not because needed"
    frequency: "Common"
    typical_cost: "Unnecessary complexity, slower development"
    solution: "Understand physics reason for pattern, evaluate if applies"
    related_patterns: ["microservices-overhead-001"]

# ============================================================================
# MEASUREMENT STANDARDS
# ============================================================================
# What counts as "validated"

validation_standards:
  high_confidence:
    requirements:
      - "Actual measurement data (profiling, monitoring)"
      - "Multiple validations (>3 systems)"
      - "Physics explanation identified"
      - "Quantitative values recorded"
    confidence_threshold: 0.90
  
  medium_confidence:
    requirements:
      - "At least one measured validation"
      - "Physics basis understood"
      - "Qualitative evidence"
    confidence_threshold: 0.70
  
  low_confidence:
    requirements:
      - "Hypothesis based on physics"
      - "Not yet measured in real system"
      - "Needs validation"
    confidence_threshold: 0.50
  
  unvalidated:
    description: "Theoretical only, no evidence"
    confidence_threshold: 0.30

# ============================================================================
# USAGE NOTES FOR AGENTS
# ============================================================================

usage_guidelines:
  for_perception_agent:
    - "Search this file first for similar constraints"
    - "Use pattern clusters to find relevant patterns quickly"
    - "Check validation_counts (higher = more reliable)"
    - "Reference seed patterns as examples"
  
  for_architecture_agent:
    - "Use architectural_recommendation from patterns"
    - "Learn from optimization_applied (what actually worked)"
    - "Check common_misconception to avoid repeating mistakes"
  
  for_execution_agent:
    - "Compare measured values to physics_value"
    - "Large deviation = potential measurement error or special case"
    - "Use measurement_data as reference ranges"
  
  for_adaptation_agent:
    - "After each validation, append to learning_history"
    - "Update pattern_validation_counts for confirmed patterns"
    - "Add new patterns when novel situations discovered"
    - "Store to Cognee memory for cross-session learning"
  
  pattern_lifecycle:
    discovery: "New pattern emerges from validation"
    validation: "Pattern tested in multiple systems"
    confirmation: "Pattern proven reliable (count >5)"
    refinement: "Pattern details improved through usage"
    promotion: "High-confidence pattern becomes seed pattern"

# ============================================================================
# META-LEARNING METRICS
# ============================================================================
# Track how the system itself improves over time

system_learning_metrics:
  # These get updated by Adaptation Agent
  total_validations: 0
  total_patterns_discovered: 0
  average_f_score_improvement: 0.0
  average_delusion_reduction: 0.0
  
  # Target metrics
  target_total_validations: 100  # After 100 validations, system is "trained"
  target_pattern_library_size: 50  # Aim for 50 validated patterns
  
  # Learning velocity (how fast system improves)
  learning_velocity: 0.0  # ΔF / validation
  
  # Most impactful patterns (updated as system learns)
  highest_impact_patterns: []
  most_common_delusions: []

# ============================================================================
# EXPORT FOR TRAINING
# ============================================================================
# After sufficient validations, export for training new Foundation Agents

export_ready: false
export_criteria:
  minimum_validations: 50
  minimum_patterns: 30
  minimum_confidence: 0.85
  
# When criteria met, this data can train new instances or update base knowledge